# COMP680_stats_for_data_science

## COURSE DESCRIPTION:
In this course, students learn the fundamentals of probability and statistical inference and their applications in real-world contexts. Simulation based inference is essential for developing core skills in data science and for basic understanding of regression-based modeling. This course is designed to help students gain a foundational knowledge of inference through the simulation process using Python programming.

## COURSE OVERVIEW:
This is one of the six core courses for the curriculum of Master of Data Science (MDS) program. This course covers the fundamentals and principles of probability and modern statistical inference with the focus of their applications in real-world contexts. Probability and statistics are essential tools in data science and central to fields like bioinformatics, social informatics, and machine learning. They are the foundation for quantifying uncertainty and assessing support for hypotheses and derived models, and are at the heart of areas such as efficiency analysis of algorithms and randomized algorithms. Course content includes probability and random variables, basic statistical concepts, and various methods for statistical inference and regression modeling. This course lays the groundwork for more advanced courses in the MDS program, such as Statistical Machine Learning and Deep Learning. 


## COURSE OBJECTIVES & LEARNING OUTCOMES:
In this course, students will learn how to summarize, explore and model data to extract useful information and knowledge from data. Students completing this course will be able to:
- Identify and calculate appropriate summary and inferential statistics and infer appropriate conclusions based on data. 
- Gain fluency in basic programming skills in Python with a focus on simulation-based inference and statistical modeling.
- Use applied statistical knowledge to analyze real-world data, test hypotheses, build regression models, and make scientific inference.
- Interpret inferences and modeling results in real-world contexts and communicate the findings effectively.


## TEXTBOOK AND OTHER RESOURCES:
- Textbooks Recommended:
  - [All of Statistics: a concise course in statistical inference](https://link.springer.com/book/10.1007/978-0-387-21736-9)
  - [An Introduction to Statistical Learning](https://www.statlearning.com/)
  - [Computer age statistical inference](https://hastie.su.domains/CASI_files/PDF/casi.pdf)
  - [Probability and Statistics for Data Science](https://cims.nyu.edu/~cfgranda/pages/stuff/probability_stats_for_DS.pdf)
- Software: Python 3 with the standard modules from an Anaconda installation such as Numpy, Pandas and Matplotlib, as well as some statistics and machine learning modules.


## COURSE SCHEDULE AND COURSE MATERIAL

| Weekly Module | Topics | lecture_slides | code_demo | hw_assignment |
|:----------|:------------------------------|:----------|:----------|:----------|
|1. Probability I  | <ul><li>probability theorey and random variables</li> <li>bivariate, marginal and conditional distributions</li></ul> | <ul><li>wk1_intro.pdf</li></ul> | <ul><li>demo01.zip</li></ul> | <ul><li>install python3 and required modules through Anaconda suite</li></ul> |
|2. Probability II  | <ul><li>multivariate random variables and Gaussian random vectors</li> <li>expectations, variance, and covariance</li> <li>probability inequalities</li></ul> | <ul><li>wk2_data_basics.pdf</li></ul> | <ul><li>demo02.zip</li></ul> | <ul><li>lab01.zip</li></ul> |
|3. Statistical Inference | <ul><li>population and random sampling</li> <li>empirical and sampling distribution</li> <li>Law of Large Numbers and Central Limit Theorem</li></ul> | <ul><li>wk3_Python_basics.pdf</li></ul> | <ul><li>demo03.zip</li></ul> | <ul><li>lab03.zip</li></ul> |
|4. Parametric Inference and MLE | <ul><li>point estimate and confidence intervals</li> <li>the likelihood functions</li> <li>MLE and its properties</li></ul> | <ul><li>demo04.zip</li></ul> | <ul><li>lab03.zip</li></ul> | 
|5. Nonparametric Inference | <ul><li>parametric and nonparametric bootstrap</li> <li>bootstrap confidence interval</li> <li>kernel density estimate</li> <li>join dataframes</li></ul> | <ul><li>wk5_Pandas_II.pdf</li></ul> | <ul><li>demo05.zip</li></ul> | <ul><li>lab04.zip</li></ul> | 
|6. Hypothesis Testing I | <ul><li>general framework of testing</li> <li>p-values and error probabilities</li> <li>common parametric tests</li></ul> | <ul><li>wk6_data_visualization.pdf</li></ul> | <ul><li>demo06.zip</li></ul> | <ul><li>lab05.zip</li></ul> |
|7. Hypothesis Testing II | <ul><li>likelihood ratio test</li> <li>goodness of fit test</li> <li>permutation test</li> <li>multiple testing and FDR control</li></ul> |  <ul><li>wk7_exploratory_data_analysis.pdf</li></ul> | <ul><li>demo07.zip</li></ul> | <ul><li>lab06.ipynb</li></ul> |
|8. Midterm Exam| <ul><li> midterm review and buff time </li></ul> |  |  |  |  |
|9. Bayesian Inference | <ul><li> the Bayesian method</li> <li> conjugate families and non-informative priors</li> <li>posterior inference</li> <li>empirical Bayes</li></ul> | <ul><li>wk9_prob_and_stats.pdf</li></ul> | <ul><li>demo09.zip</li></ul> | <ul><li>lab07.zip</li></ul> |
|10. Stochastic Processes | <ul><li> Markov Chains</li> <li>Poisson Processes</li></ul> | <ul><li>wk10_hypothesis_testing.pdf</li></ul> | <ul><li>demo10.zip</li></ul> | <ul><li>lab08.zip</li></ul> |
|11. Linear Regression | <ul><li>correlation and simple linear regression</li> <li>least square and maximum likelihood</li> <li>multiple linear regression</li> <li>model comparison and selection</li></ul>| <ul><li>wk11_correlation_regression.pdf</li></ul> | <ul><li>demo11.zip</li></ul> | <ul><li>lab09.zip</li></ul> |
|12. Generalized Linear Models | <ul><li>logistic regression</li> <li>multinomial and ordinal regression</li> <li>poisson regression</li></ul> |  <ul><li>wk12_intro_to_ml.pdf</li></ul> | <ul><li>demo12.zip</li></ul> | <ul><li>lab10.zip</li></ul> |
|13. Regularized and nonlinear regression |  <ul><li>ridge regression</li> <li>lasso regression </li> <li> nonlinear regression with splines </li></ul> |  <ul><li>wk13_decision_trees.pdf</li></ul> | <ul><li>demo13.zip</li></ul> | <ul><li>lab10.zip</li></ul> |
|14. Generalized additive models |  <ul><li> generalized additive models </li> <li> Gaussian mixture models</li></ul> |  <ul><li>wk14_unsupervised_learning.pdf</li></ul> | <ul><li>demo14.zip</li></ul> | <ul><li>lab11.zip</li></ul> |
|15. Final Exam | <ul><li> final review and buff time </li></ul> |  |  |  |  |

## FOR THOSE WHO INTERESTED:
- The posted lecture slides only serve as high-level conceptual introduction for the topics covered in the weekly module. During class, more material will be added on the slides including: proofs and derivations, examples, summaries and reviews, etc. 
- The code demo notebooks will be ran during class with some explaination of code and interpretation of the results. 
- Homework assignments are bi-weekly with a combination of mathemetical derivation, coding exercise, and real data analysis. 
- The following materials are avalible upon request, please contact me if you are interested: 
  - Slides in Latex format.
  - Completed slides with hand-written notes in class. 
  - Homework assignment solutions and grading rubrics.
  - Sample exam solutions.
